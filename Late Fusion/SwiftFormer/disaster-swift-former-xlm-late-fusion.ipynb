{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-14T03:28:16.772880Z",
     "iopub.status.busy": "2024-07-14T03:28:16.772185Z",
     "iopub.status.idle": "2024-07-14T03:28:17.205964Z",
     "shell.execute_reply": "2024-07-14T03:28:17.205058Z",
     "shell.execute_reply.started": "2024-07-14T03:28:16.772844Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths to the CSV files and image directories\n",
    "csv_paths = {\n",
    "    'Train': '/kaggle/input/disaster/train.csv',\n",
    "    'Test': '/kaggle/input/disaster/test.csv',\n",
    "    'Validation': '/kaggle/input/disaster/validation.csv'\n",
    "}\n",
    "\n",
    "image_dirs = {\n",
    "    'Train': '/kaggle/input/disaster/train',\n",
    "    'Test': '/kaggle/input/disaster/test',\n",
    "    'Validation': '/kaggle/input/disaster/validation'\n",
    "}\n",
    "\n",
    "output_dir = '/kaggle/working/'  # Output directory to save the CSV files\n",
    "\n",
    "# Function to check for matching Meme_ID and image files, and add image paths\n",
    "def check_matches(csv_path, image_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    image_files = os.listdir(image_dir)\n",
    "    image_names = {os.path.splitext(image_file)[0]: os.path.join(image_dir, image_file) for image_file in image_files}\n",
    "    \n",
    "    # Add Image_Path column to the dataframe\n",
    "    df['Image_Path'] = df['image_id'].apply(lambda x: image_names.get(x, None))\n",
    "    \n",
    "    # Filter rows where Image_Path is not None (i.e., matched Meme_IDs)\n",
    "    matched_df = df[df['Image_Path'].notna()]\n",
    "    \n",
    "    return matched_df\n",
    "\n",
    "# Check matches for each set (Train, Test, Validation)\n",
    "for key in csv_paths:\n",
    "    matched_df = check_matches(csv_paths[key], image_dirs[key])\n",
    "    \n",
    "    matches_output_path = os.path.join(output_dir, f'{key}_matches.csv')\n",
    "    \n",
    "    matched_df.to_csv(matches_output_path, index=False)\n",
    "    \n",
    "    print(f\"{key} set:\")\n",
    "    print(f\"Matched Meme_IDs with image paths saved to {matches_output_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:28:20.358038Z",
     "iopub.status.busy": "2024-07-14T03:28:20.357691Z",
     "iopub.status.idle": "2024-07-14T03:28:20.380435Z",
     "shell.execute_reply": "2024-07-14T03:28:20.379431Z",
     "shell.execute_reply.started": "2024-07-14T03:28:20.358009Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/working/Train_matches.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:28:22.142467Z",
     "iopub.status.busy": "2024-07-14T03:28:22.141703Z",
     "iopub.status.idle": "2024-07-14T03:28:22.156351Z",
     "shell.execute_reply": "2024-07-14T03:28:22.155425Z",
     "shell.execute_reply.started": "2024-07-14T03:28:22.142427Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/kaggle/working/Test_matches.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:28:24.419798Z",
     "iopub.status.busy": "2024-07-14T03:28:24.419465Z",
     "iopub.status.idle": "2024-07-14T03:28:24.433978Z",
     "shell.execute_reply": "2024-07-14T03:28:24.432893Z",
     "shell.execute_reply.started": "2024-07-14T03:28:24.419771Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_df = pd.read_csv('/kaggle/working/Validation_matches.csv')\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:28:26.395975Z",
     "iopub.status.busy": "2024-07-14T03:28:26.395361Z",
     "iopub.status.idle": "2024-07-14T03:28:26.400937Z",
     "shell.execute_reply": "2024-07-14T03:28:26.399930Z",
     "shell.execute_reply.started": "2024-07-14T03:28:26.395941Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display the sizes of each split\n",
    "print(f\"Train dataset size: {len(train_df)}\")\n",
    "print(f\"Test dataset size: {len(test_df)}\")\n",
    "print(f\"Validation dataset size: {len(validation_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:28:28.948872Z",
     "iopub.status.busy": "2024-07-14T03:28:28.948521Z",
     "iopub.status.idle": "2024-07-14T03:28:28.991963Z",
     "shell.execute_reply": "2024-07-14T03:28:28.991116Z",
     "shell.execute_reply.started": "2024-07-14T03:28:28.948842Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Function to remove punctuation (preserve Bangla characters)\n",
    "def remove_punctuation(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to remove extra whitespace\n",
    "def remove_whitespace(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "# Function to remove emojis\n",
    "def remove_emojis(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Function to remove URLs\n",
    "def remove_urls(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "# Function to remove HTML tags\n",
    "def remove_html(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    html_pattern = re.compile(r'<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "# Function to remove special characters (preserve Bangla characters)\n",
    "def remove_special_characters(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return re.sub(r'[^A-Za-z0-9\\s\\u0980-\\u09FF]', '', text)\n",
    "\n",
    "# Combine all cleaning functions\n",
    "def clean_text(text):\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_whitespace(text)\n",
    "    return text\n",
    "\n",
    "# Mapping categories to integers\n",
    "category_mapping = {\n",
    "    'Landslides': 0,\n",
    "    'Wildfire': 1,\n",
    "    'Drought': 2,\n",
    "    'Flood': 3,\n",
    "    'Earthquake': 4,\n",
    "    'Tropical Storm': 5,\n",
    "    'Non Disaster': 6,\n",
    "    'Human Damage': 7,\n",
    "}\n",
    "\n",
    "# Load and clean the dataframes\n",
    "csv_paths = {\n",
    "    'Train': '/kaggle/working/Train_matches.csv',\n",
    "    'Test': '/kaggle/working/Test_matches.csv',\n",
    "    'Validation': '/kaggle/working/Validation_matches.csv'\n",
    "}\n",
    "\n",
    "cleaned_output_paths = {\n",
    "    'Train': '/kaggle/working/Train_matches_cleaned.csv',\n",
    "    'Test': '/kaggle/working/Test_matches_cleaned.csv',\n",
    "    'Validation': '/kaggle/working/Validation_matches_cleaned.csv'\n",
    "}\n",
    "\n",
    "text_columns = ['context', 'category']\n",
    "\n",
    "for key in csv_paths:\n",
    "    # Load the dataframe\n",
    "    df = pd.read_csv(csv_paths[key])\n",
    "    \n",
    "    # Apply cleaning to all relevant text columns\n",
    "    for column in text_columns:\n",
    "        df[column] = df[column].astype(str).apply(clean_text)\n",
    "    \n",
    "    # Map 'category' column to integers\n",
    "    df['category'] = df['category'].map(category_mapping)\n",
    "    \n",
    "    # Add 'label' column (same as 'category' for now)\n",
    "    df['label'] = df['category']\n",
    "    \n",
    "    # Display the cleaned dataframe\n",
    "    print(f\"Cleaned {key} dataframe:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Save the cleaned dataframe to a new CSV file\n",
    "    df.to_csv(cleaned_output_paths[key], index=False)\n",
    "    print(f\"Cleaned dataframe saved to {cleaned_output_paths[key]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:28:33.303345Z",
     "iopub.status.busy": "2024-07-14T03:28:33.302709Z",
     "iopub.status.idle": "2024-07-14T03:28:33.321597Z",
     "shell.execute_reply": "2024-07-14T03:28:33.320582Z",
     "shell.execute_reply.started": "2024-07-14T03:28:33.303313Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/working/Train_matches_cleaned.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:28:35.039973Z",
     "iopub.status.busy": "2024-07-14T03:28:35.039625Z",
     "iopub.status.idle": "2024-07-14T03:28:35.053722Z",
     "shell.execute_reply": "2024-07-14T03:28:35.052739Z",
     "shell.execute_reply.started": "2024-07-14T03:28:35.039943Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/kaggle/working/Test_matches_cleaned.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:28:37.328920Z",
     "iopub.status.busy": "2024-07-14T03:28:37.328236Z",
     "iopub.status.idle": "2024-07-14T03:28:37.342770Z",
     "shell.execute_reply": "2024-07-14T03:28:37.341826Z",
     "shell.execute_reply.started": "2024-07-14T03:28:37.328886Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_df = pd.read_csv('/kaggle/working/Validation_matches_cleaned.csv')\n",
    "validation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:28:39.643195Z",
     "iopub.status.busy": "2024-07-14T03:28:39.642278Z",
     "iopub.status.idle": "2024-07-14T03:28:45.331398Z",
     "shell.execute_reply": "2024-07-14T03:28:45.330595Z",
     "shell.execute_reply.started": "2024-07-14T03:28:39.643162Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:28:45.333215Z",
     "iopub.status.busy": "2024-07-14T03:28:45.332796Z",
     "iopub.status.idle": "2024-07-14T03:29:09.326732Z",
     "shell.execute_reply": "2024-07-14T03:29:09.325586Z",
     "shell.execute_reply.started": "2024-07-14T03:28:45.333188Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:37:26.860798Z",
     "iopub.status.busy": "2024-07-14T03:37:26.860424Z",
     "iopub.status.idle": "2024-07-14T03:37:29.069728Z",
     "shell.execute_reply": "2024-07-14T03:37:29.068820Z",
     "shell.execute_reply.started": "2024-07-14T03:37:26.860767Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, SwiftFormerModel\n",
    "import torch\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MBZUAI/swiftformer-xs\")\n",
    "\n",
    "model = SwiftFormerModel.from_pretrained(\"MBZUAI/swiftformer-xs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:37:32.490075Z",
     "iopub.status.busy": "2024-07-14T03:37:32.489333Z",
     "iopub.status.idle": "2024-07-14T03:37:33.407993Z",
     "shell.execute_reply": "2024-07-14T03:37:33.407213Z",
     "shell.execute_reply.started": "2024-07-14T03:37:32.490037Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, XLMRobertaModel, AdamW\n",
    "# Initialize BERT tokenizer and model\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "bert_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:37:39.480864Z",
     "iopub.status.busy": "2024-07-14T03:37:39.480182Z",
     "iopub.status.idle": "2024-07-14T03:37:39.485830Z",
     "shell.execute_reply": "2024-07-14T03:37:39.484862Z",
     "shell.execute_reply.started": "2024-07-14T03:37:39.480831Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:37:42.967209Z",
     "iopub.status.busy": "2024-07-14T03:37:42.966362Z",
     "iopub.status.idle": "2024-07-14T03:37:42.973429Z",
     "shell.execute_reply": "2024-07-14T03:37:42.972640Z",
     "shell.execute_reply.started": "2024-07-14T03:37:42.967178Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Enable device-side assertions\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:37:45.017986Z",
     "iopub.status.busy": "2024-07-14T03:37:45.017172Z",
     "iopub.status.idle": "2024-07-14T03:37:45.041941Z",
     "shell.execute_reply": "2024-07-14T03:37:45.041092Z",
     "shell.execute_reply.started": "2024-07-14T03:37:45.017957Z"
    }
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:37:49.298505Z",
     "iopub.status.busy": "2024-07-14T03:37:49.298116Z",
     "iopub.status.idle": "2024-07-14T03:37:49.495095Z",
     "shell.execute_reply": "2024-07-14T03:37:49.494322Z",
     "shell.execute_reply.started": "2024-07-14T03:37:49.298473Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:37:52.744250Z",
     "iopub.status.busy": "2024-07-14T03:37:52.743436Z",
     "iopub.status.idle": "2024-07-14T03:37:52.755433Z",
     "shell.execute_reply": "2024-07-14T03:37:52.754432Z",
     "shell.execute_reply.started": "2024-07-14T03:37:52.744215Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "max_seq_length = 512  # Set your desired maximum sequence length for BERT\n",
    "\n",
    "# Define the pre-processing transformations for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class MyMultimodalDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, tokenizer=None, max_seq_length=512):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data.iloc[idx]['Image_Path']\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image at index {idx}: {e}\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        if image is None:\n",
    "            return None, None, None, None\n",
    "\n",
    "        context = self.data.iloc[idx]['context']\n",
    "\n",
    "        inputs = self.tokenizer(context, padding='max_length', truncation=True, max_length=self.max_seq_length, return_tensors='pt')\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        return image, input_ids, attention_mask, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:37:55.546262Z",
     "iopub.status.busy": "2024-07-14T03:37:55.545373Z",
     "iopub.status.idle": "2024-07-14T03:37:55.559975Z",
     "shell.execute_reply": "2024-07-14T03:37:55.558960Z",
     "shell.execute_reply.started": "2024-07-14T03:37:55.546215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create custom datasets with MyMultimodalDataset\n",
    "train_dataset = MyMultimodalDataset(train_df, transform=transform, tokenizer=bert_tokenizer, max_seq_length=max_seq_length)\n",
    "test_dataset = MyMultimodalDataset(test_df, transform=transform, tokenizer=bert_tokenizer, max_seq_length=max_seq_length)\n",
    "val_dataset = MyMultimodalDataset(validation_df, transform=transform, tokenizer=bert_tokenizer, max_seq_length=max_seq_length)\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 1  # Set your desired batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:39:22.604780Z",
     "iopub.status.busy": "2024-07-14T03:39:22.604347Z",
     "iopub.status.idle": "2024-07-14T03:39:24.057609Z",
     "shell.execute_reply": "2024-07-14T03:39:24.056666Z",
     "shell.execute_reply.started": "2024-07-14T03:39:22.604748Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from transformers import AutoImageProcessor, ViTModel, AutoTokenizer, DistilBertModel, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Assuming you have defined your train_loader, val_loader, optimizer, criterion, model, bert_model, etc.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the regressor model and optimizer\n",
    "regressor = torch.nn.Sequential(\n",
    "    torch.nn.Linear(11548, 512),  # Adjusted input dimension\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.5),\n",
    "    torch.nn.Linear(512, 8)\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(regressor.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 40\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training time\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    regressor.train()\n",
    "\n",
    "    for images, texts, attention_masks, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        input_ids = texts.squeeze(1).to(device)\n",
    "        attention_mask = attention_masks.squeeze(1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get image features\n",
    "        with torch.no_grad():\n",
    "            outputs_image = model(pixel_values=images)\n",
    "        img_hidden_states = outputs_image.last_hidden_state\n",
    "        # Flatten the image features\n",
    "        img_feats = img_hidden_states.view(img_hidden_states.size(0), -1)\n",
    "        print(f'Image features shape: {img_feats.shape}')  # Debug print\n",
    "\n",
    "        # Get text features\n",
    "        outputs_text = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_hidden_states = outputs_text.last_hidden_state\n",
    "        text_feats = text_hidden_states[:, 0, :]\n",
    "        print(f'Text features shape: {text_feats.shape}')  # Debug print\n",
    "\n",
    "        # Concatenate image and text features\n",
    "        combined_feats = torch.cat((img_feats, text_feats), dim=1)\n",
    "        print(f'Combined features shape: {combined_feats.shape}')  # Debug print\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = regressor(combined_feats)\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_loader)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {epoch_train_loss:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:40:26.572530Z",
     "iopub.status.busy": "2024-07-14T03:40:26.572141Z",
     "iopub.status.idle": "2024-07-14T03:40:27.190700Z",
     "shell.execute_reply": "2024-07-14T03:40:27.189855Z",
     "shell.execute_reply.started": "2024-07-14T03:40:26.572499Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Set models to evaluation mode\n",
    "model.eval()\n",
    "bert_model.eval()\n",
    "\n",
    "# Prepare lists to store predicted and true labels\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "# Set your device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Test loop\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_images, test_texts, test_attention_masks, test_labels in tqdm(test_loader, desc='Testing', leave=False):\n",
    "        test_images = test_images.to(device)\n",
    "        test_labels = (test_labels).to(device)  # Convert labels from 1-5 to 0-4\n",
    "        test_texts = test_texts.to(device)\n",
    "        test_attention_masks = test_attention_masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Extract features from image-based model\n",
    "        test_img_feats = model(test_images)\n",
    "\n",
    "        # Extract features from text-based model (BERT)\n",
    "        test_texts = test_texts.squeeze(1)\n",
    "        test_attention_masks = test_attention_masks.squeeze(1)\n",
    "        test_outputs = bert_model(input_ids=test_texts, attention_mask=test_attention_masks)\n",
    "        \n",
    "        # Extract relevant features for concatenation\n",
    "        # Handle tensor manipulation for compatibility\n",
    "        # Reshape test_img_feats if compatible\n",
    "        test_img_feats = test_img_feats[0]  # Access a specific part of the BaseModelOutputWithNoAttention\n",
    "        test_img_feats = test_img_feats.reshape(test_img_feats.shape[0], -1)  # Reshape if compatible\n",
    "\n",
    "        # Extract representations from text-based model\n",
    "        test_text_feats = test_outputs.last_hidden_state[:, 0, :]  # Select appropriate representations\n",
    "\n",
    "        # Combine features early\n",
    "        combined_feats = torch.cat((test_img_feats, test_text_feats), dim=1)\n",
    "\n",
    "        # Classify combined features\n",
    "        combined_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(combined_feats.shape[1], 512).to(device),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(512, 8).to(device),  # Change the output size to 5 for 5 labels\n",
    "        )\n",
    "\n",
    "        combined_logits = combined_classifier(combined_feats)\n",
    "        test_predictions = torch.nn.functional.softmax(combined_logits, dim=1)\n",
    "        predicted_classes = torch.argmax(test_predictions, dim=1)  # Revert back to labels from 1-5, Add 1 here \n",
    "\n",
    "        predicted_labels.extend(predicted_classes.cpu().numpy())\n",
    "        true_labels.extend(test_labels.cpu().numpy().tolist())\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print or use the predicted labels and true labels as needed\n",
    "print(\"Predicted Labels:\", predicted_labels)\n",
    "print(\"True Labels:\", true_labels)\n",
    "print(f\"Total execution time for testing: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:40:31.547991Z",
     "iopub.status.busy": "2024-07-14T03:40:31.547211Z",
     "iopub.status.idle": "2024-07-14T03:40:31.553382Z",
     "shell.execute_reply": "2024-07-14T03:40:31.552416Z",
     "shell.execute_reply.started": "2024-07-14T03:40:31.547957Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:40:33.563602Z",
     "iopub.status.busy": "2024-07-14T03:40:33.562951Z",
     "iopub.status.idle": "2024-07-14T03:40:33.568978Z",
     "shell.execute_reply": "2024-07-14T03:40:33.568117Z",
     "shell.execute_reply.started": "2024-07-14T03:40:33.563568Z"
    }
   },
   "outputs": [],
   "source": [
    "true_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T03:40:37.508803Z",
     "iopub.status.busy": "2024-07-14T03:40:37.508193Z",
     "iopub.status.idle": "2024-07-14T03:40:37.524337Z",
     "shell.execute_reply": "2024-07-14T03:40:37.523120Z",
     "shell.execute_reply.started": "2024-07-14T03:40:37.508771Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, mean_squared_error, classification_report\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate precision, recall, F1-score overall (macro average)\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate Sensitivity (Recall) for each class\n",
    "sensitivity_per_class = recall\n",
    "\n",
    "# Calculate Specificity for each class\n",
    "specificity_per_class = []\n",
    "for i in range(len(conf_matrix)):\n",
    "    tn = np.sum(conf_matrix) - (np.sum(conf_matrix[i, :]) + np.sum(conf_matrix[:, i]) - conf_matrix[i, i])\n",
    "    fp = np.sum(conf_matrix[:, i]) - conf_matrix[i, i]\n",
    "    specificity_per_class.append(tn / (tn + fp))\n",
    "\n",
    "# Print overall calculated metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision (macro): {precision}\")\n",
    "print(f\"Recall (macro): {recall}\")\n",
    "print(f\"F1-Score (macro): {f1_score}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Print Sensitivity and Specificity for each class\n",
    "print(f\"Sensitivity (Recall) for each class: {sensitivity_per_class}\")\n",
    "print(f\"Specificity for each class: {specificity_per_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5384900,
     "sourceId": 8948385,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
